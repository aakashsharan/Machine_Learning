{"cells":[{"cell_type":"markdown","source":["Lets create a Word Count Application. We will write code to calculate the most common words in [A Tale of Two Cities by Charles Dickens](https://www.gutenberg.org/ebooks/98) from Project Gutenberg.   \nBefore we read the data, we are going to create a sample data frame and write functions that count words, remove punctuations. We would test those functions and once everything works fine we read   \nthe data and create a full application."],"metadata":{}},{"cell_type":"markdown","source":["Lets create a DataFrame by using a Python list of tuples.   \nThen we print the data and schema. But before all of this, we will check spark version and type of sqlContext."],"metadata":{}},{"cell_type":"code","source":["print sc.version\nprint type(sqlContext)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dataDF = sqlContext.createDataFrame([('Jax',), ('Rammus',), ('Zac',), ('Xin', ), ('Hecarim', ), ('Zac', ), ('Rammus', )], ['Jungler'])\ndataDF.show()\nprint type(dataDF)\ndataDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# lets create a new DataFrame by adding a 'y' to each word.\n# to concatenate we would be using concat and lit function.\nfrom pyspark.sql.functions import lit, concat\n\nnewDataDF = dataDF.select(concat(dataDF.Jungler, lit('y')).alias('jungler'))\nnewDataDF.show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# lets find the number of characters in each word.\nfrom pyspark.sql.functions import length\nnewDataLengthDF = newDataDF.select(length('jungler'))\nnewDataLengthDF.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Lets perform aggregations by grouping data on the DataFrame by using groupBy function.   \nUsing groupBy lets find the counts of words."],"metadata":{}},{"cell_type":"code","source":["dataCountsDF = (dataDF\n                .groupBy('Jungler')\n                .count()\n                )\ndataCountsDF.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Lets define a function for word counting. Lets use the technique that we implemented earlier."],"metadata":{}},{"cell_type":"code","source":["def wordCount(wordsDF):\n  \"\"\" Creates a DataFrame with word counts.\n  \n  Args:\n      wordsDF: A DataFrame consisting of one string column called 'Jungler'.\n      \n  Returns:\n      DataFrame of a (str, int): containing Jungler and count columns.\n  \"\"\"\n  return (wordsDF\n           .groupBy('Jungler')\n           .count())\n\nwordCount(dataDF).show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["If we have a large notebook and it is getting difficult to find the DataFrame and their schema, Spark provides a very useful function printDataFrames to find   \nall the DataFrame created so far."],"metadata":{}},{"cell_type":"code","source":["from spark_notebook_helpers import printDataFrames\nprintDataFrames(True)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# calculate number of unique junglers\nuniqueJunglers = wordCount(dataDF).count()\nprint uniqueJunglers\n\n# calculate mean number of occurences\naverageCount = round(((dataCountsDF.groupBy().mean('count').collect())[0][0]), 2)\nprint averageCount"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Lets do some data wrangling operation. As we are going to work on dataset of text files, we would keep the following assumptions:\n+ words should be counted independent of their capitalization.\n+ punctuation should be removed.\n+ leading and trailing spaces should be removed.\n\nTo keep the above assumptions intact, we create a function called removePunctuation to perform the three operations."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace, trim, col, lower\ndef removePunctuation(column):\n  \"\"\"Removes punctuation, changes to lower case, strips leading and trailing spaces.\n  \n  Args:\n    column: a sentence\n  \n  Returns:\n    Column: a column named 'sentence'\n  \"\"\"\n  \n  return lower(trim(regexp_replace(column, '[^\\w\\s]+', \"\"))).alias('sentence')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# test removePunctuation operation\nsentenceDF = sqlContext.createDataFrame([('Hello, World!', ), (' How about some ice-cream?',), ('  *  Well, how about a cookie and lots of candies? *',)], ['sentence'])\nsentenceDF.show(truncate=False)\n(sentenceDF\n  .select(removePunctuation(col('sentence')))\n  .show(truncate=False))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["We have both the functions ready, lets read the dataset and perform the operations accordingly."],"metadata":{}},{"cell_type":"code","source":["filename = \"/FileStore/tables/tux1rb3k1481050985024/TaleOfTwoCities.txt\"\ntwoCitiesDF = sqlContext.read.text(filename).select(removePunctuation(col('value')))\ntwoCitiesDF.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Before we use the wordCount() function, we have to address the following issues:\n+ Need to split each line by spaces.\n+ Filter out empty lines or words.   \nLets address these two issues first."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import split,explode\ntwoCitiesWordsDF = (twoCitiesDF\n                    .select(explode(split(twoCitiesDF.sentence, \" \")).alias('word'))\n                    .where(\"word != ''\")\n                    )\ntwoCitiesWordsDF.show(truncate=False)\ntwoCitiesWordsDFCount = twoCitiesWordsDF.count()\nprint twoCitiesWordsDFCount"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["One more issue to address. We need to refactor wordCount() in order to groupby word"],"metadata":{}},{"cell_type":"code","source":["def wordCount(wordsDF):\n  \"\"\" Creates a DataFrame with word counts.\n  \n  Args:\n      wordsDF: A DataFrame consisting of one string column called 'Jungler'.\n      \n  Returns:\n      DataFrame of a (str, int): containing Jungler and count columns.\n  \"\"\"\n  return (wordsDF\n           .groupBy('word')\n           .count())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# produce a list of words in descending order.\nfrom pyspark.sql.functions import desc\ntopWordsAndCountsDF = wordCount(twoCitiesWordsDF).orderBy(desc('count'))\ntopWordsAndCountsDF.show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["We see a lot of stop words like a, the, of etc in the top 20 words.  \nWe will keep those for now, will remove them in the next project."],"metadata":{}}],"metadata":{"name":"Word_Count_Application","notebookId":1600547075740304},"nbformat":4,"nbformat_minor":0}
