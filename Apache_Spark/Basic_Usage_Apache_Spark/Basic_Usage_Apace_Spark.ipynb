{"cells":[{"cell_type":"markdown","source":["Basica notebook usage. We can write markdown or code in the cells. Use \"Shift\" + \"Enter\" to execute the code and advance to the next cell. Use \"Ctrl\" + \"Enter\" to exectute the code and remain in the cell.    \nHere we are going to write a simple python code and run it."],"metadata":{}},{"cell_type":"code","source":["# Lets run some python code.\na = 10\nb = -20\nif a < b:\n  print 'b is greater'\nelse:\n  print 'a is greater'"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["In order to use Spark and its DataFrame API we will need to use SQLContext.\nWhen running Spark, we start a new application by creating SparkContext and then we create a SQLContext from SparkContext."],"metadata":{}},{"cell_type":"code","source":["# check what version of Spark are we running\nsc.version"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# display the type of the Spark sqlContext\ntype(sqlContext)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Before we create a DataFrame, lets use a Python package called fake-factory to create fake person records.    \nFor our project, we are going to create 10,000 records wih columns - last_name, first_name, occupation, company, age."],"metadata":{}},{"cell_type":"code","source":["# Lets create a python collection of 10,000 people\nfrom faker import Factory\nfake = Factory.create()\nfake.seed(1234)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import Row\ndef fake_data():\n  name = fake.name().split()\n  return(name[1], name[0], fake.job(), fake.company(), abs(2016 - fake.date_time().year) + 1)\n\n\n# xrange is evaluated lazily and acts like a generator while range creates a list in memory.\ndef repeat_times(times, func, *args, **kwargs):\n  for _ in xrange(times):\n    yield func(*args, **kwargs)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["data = list(repeat_times(10000, fake_data))\ndata[0]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["len(data)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# create DataFrame\ndataDF = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'occupation', 'company', 'age'))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# register the dataframe as a named table called person.\nsqlContext.registerDataFrameAsTable(dataDF, 'person')"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["dataDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["dataDF.show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["dataDF.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["subDF = dataDF.select('last_name', 'first_name')\nsubDF.show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# collec to view results. Not a good idea if the dataset is large.\nresults = subDF.collect()\nprint results"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["dataDF.show(n=30, truncate=False)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# a better way to visualize data.\ndisplay(dataDF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["print dataDF.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# lets apply another transformation, filter.\nfilteredDF = dataDF.filter(dataDF.age < 13)\nfilteredDF.show(truncate=False)\nfilteredDF.count()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# python lambda functions and udf\nfrom pyspark.sql.types import BooleanType\nless_13 = udf(lambda s: s < 13, BooleanType())\nlambdaDF = dataDF.filter(less_13(dataDF.age))\nlambdaDF.show()\nlambdaDF.count()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# lets collect the even values less than 10.\neven_less13 = udf(lambda s: s%2 == 0, BooleanType())\nevenDF = lambdaDF.filter(even_less13(lambdaDF.age))\nevenDF.show()\nevenDF.count()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(evenDF.take(5))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(dataDF.orderBy(dataDF.age.desc()).take(5))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(dataDF.orderBy('age').take(5))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# we can use groupby (similar like pandas)\ndataDF.groupBy('occupation').count().show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["dataDF.groupBy().avg('age').show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["dataDF.groupBy().max('age').show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["We can use cache() operation to keep the DataFrame in memory.   \ncache() is lazy so before running the operation we need to make sure we call an action operation."],"metadata":{}},{"cell_type":"code","source":["dataDF.cache()\nprint dataDF.is_cached"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["dataDF.unpersist()\nprint dataDF.is_cached"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# readability and code style\nfrom pyspark.sql.functions import *\n(dataDF\n .filter(dataDF.age > 25)\n .select(concat(dataDF.first_name, lit(' '), dataDF.last_name).alias('name'), dataDF.occupation, dataDF.age)\n .show(truncate=False)\n)"],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"Basic_Usage_Apace_Spark","notebookId":4084384703570759},"nbformat":4,"nbformat_minor":0}
