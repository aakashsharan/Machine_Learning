{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load testing library\n",
    "from databricks_test_helper import Test\n",
    "import os.path\n",
    "file_name = os.path.join('databricks-datasets', 'cs190', 'data-001', 'millionsong.txt')\n",
    "\n",
    "raw_data_df = sqlContext.read.load(file_name, 'text')\n",
    "#raw_data_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_points = raw_data_df.count()\n",
    "print num_points\n",
    "sample_points = raw_data_df.take(5)\n",
    "print sample_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# Here is a sample raw data point:\n",
    "# '2001.0,0.884,0.610,0.600,0.474,0.247,0.357,0.344,0.33,0.600,0.425,0.60,0.419'\n",
    "# In this raw data point, 2001.0 is the label, and the remaining values are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In MLlib, labeled training instances are stored using the LabeledPoint\n",
    "from pyspark.sql import functions as sql_functions\n",
    "\n",
    "def parse_points(df):\n",
    "    \"\"\"Converts a DataFrame of comma separated unicode strings into a DataFrame of `LabeledPoints`.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame where each row is a comma separated unicode string. The first element in the string\n",
    "            is the label and the remaining elements are the features.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Each row is converted into a `LabeledPoint`, which consists of a label and\n",
    "            features. To convert an RDD to a DataFrame, simply call toDF().\n",
    "    \"\"\"\n",
    "    return (df.select(sql_functions.split(df.value, ',')).alias('value')\n",
    "            .map(lambda row: LabeledPoint( float(row[0][0]), list(row[0][1:]) ))\n",
    "            .toDF(['features', 'label']))\n",
    "\n",
    "parsed_points_df = parse_points(raw_data_df)\n",
    "print(parsed_points_df)\n",
    "first_point_features = parsed_points_df.first().features\n",
    "first_point_label = parsed_points_df.first().label\n",
    "print first_point_features, first_point_label\n",
    "\n",
    "d = len(first_point_features)\n",
    "print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's examine the labels to find the range of song years.\n",
    "content_stats = (parsed_points_df\n",
    "                 .selectExpr('min(label)', 'max(label)')\n",
    "                .collect())\n",
    "print(content_stats)\n",
    "min_year = content_stats[0][0]\n",
    "max_year = content_stats[0][1]\n",
    "\n",
    "print min_year, max_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The labels are years in the 1900s and 2000s. it is often natural to shift labels such that they start from zero.\n",
    "parsed_data_df = (parsed_points_df.select(parsed_points_df.features, parsed_points_df.label - min_year)\n",
    "                     .withColumnRenamed('(label - ' + str(min_year) + ')', 'label')\n",
    "                      )\n",
    "# View the first point\n",
    "print '\\n{0}'.format(parsed_data_df.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization: Shifting labels\n",
    "\n",
    "We will look at the labels before and after shifting them. \n",
    "The first scatter plot uses the initial labels, while the second one uses the shifted labels.  Note that the two plots look the same except for the labels on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data for plot\n",
    "old_data = (parsed_points_df\n",
    "             .rdd\n",
    "             .map(lambda lp: (lp.label, 1))\n",
    "             .reduceByKey(lambda x, y: x + y)\n",
    "             .collect())\n",
    "x, y = zip(*old_data)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(1920, 2050, 20), np.arange(0, 150, 20))\n",
    "plt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "ax.set_xlabel('Year'), ax.set_ylabel('Count')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data for plot\n",
    "new_data = (parsed_points_df\n",
    "             .rdd\n",
    "             .map(lambda lp: (lp.label, 1))\n",
    "             .reduceByKey(lambda x, y: x + y)\n",
    "             .collect())\n",
    "x, y = zip(*new_data)\n",
    "\n",
    "# generate layout and plot data\n",
    "fig, ax = prepare_plot(np.arange(0, 120, 20), np.arange(0, 120, 20))\n",
    "plt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "ax.set_xlabel('Year (shifted)'), ax.set_ylabel('Count')\n",
    "display(fig)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spliting the dataset into training, validation and test sets\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsed_train_data_df, parsed_val_data_df, parsed_test_data_df = parsed_data_df.randomSplit(weights, seed)\n",
    "parsed_train_data_df.cache()\n",
    "parsed_val_data_df.cache()\n",
    "parsed_test_data_df.cache()\n",
    "n_train = parsed_train_data_df.count()\n",
    "n_val = parsed_val_data_df.count()\n",
    "n_test = parsed_test_data_df.count()\n",
    "\n",
    "print n_train, n_val, n_test, n_train + n_val + n_test\n",
    "print parsed_data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computation for baseline model.\n",
    "average_train_year = (parsed_train_data_df\n",
    "                        .selectExpr('avg(label)').first()[0])\n",
    "print average_train_year\n",
    "print(parsed_train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the RMSE for a given set of (prediction, label) tuples.\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "preds_and_labels = [(1., 3.), (2., 1.), (2., 2.)]\n",
    "preds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\n",
    "\n",
    "evaluator = RegressionEvaluator()\n",
    "def calc_RMSE(dataset):\n",
    "    \"\"\"Calculates the root mean squared error for an dataset of (prediction, label) tuples.\n",
    "\n",
    "    Args:\n",
    "        dataset (DataFrame of (float, float)): A `DataFrame` consisting of (prediction, label) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "    return evaluator.evaluate(dataset)\n",
    "\n",
    "example_rmse = calc_RMSE(preds_and_labels_df)\n",
    "print example_rmse\n",
    "# RMSE = sqrt[((1-3)^2 + (2-1)^2 + (2-2)^2) / 3] = 1.291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the training, validation and test RMSE of our baseline model\n",
    "preds_and_labels_train = parsed_train_data_df.map(lambda x: (average_train_year, x.label))\n",
    "preds_and_labels_train_df = sqlContext.createDataFrame(preds_and_labels_train, [\"prediction\", \"label\"])\n",
    "rmse_train_base = calc_RMSE(preds_and_labels_train_df)\n",
    "\n",
    "preds_and_labels_val = parsed_val_data_df.map(lambda x: (average_train_year, x.label))\n",
    "preds_and_labels_val_df = sqlContext.createDataFrame(preds_and_labels_val, [\"prediction\", \"label\"])\n",
    "rmse_val_base = calc_RMSE(preds_and_labels_val_df)\n",
    "\n",
    "preds_and_labels_test = parsed_test_data_df.map(lambda x: (average_train_year, x.label))\n",
    "preds_and_labels_test_df = sqlContext.createDataFrame(preds_and_labels_test, [\"prediction\", \"label\"])\n",
    "rmse_test_base = calc_RMSE(preds_and_labels_test_df)\n",
    "\n",
    "print 'Baseline Train RMSE = {0:.3f}'.format(rmse_train_base)\n",
    "print 'Baseline Validation RMSE = {0:.3f}'.format(rmse_val_base)\n",
    "print 'Baseline Test RMSE = {0:.3f}'.format(rmse_test_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient summand\n",
    "\n",
    "Now let's see if we can do better via linear regression, training a model via gradient descent (we'll omit the intercept for now). Recall that the gradient descent update for linear regression is:\n",
    "\\\\[ \\scriptsize \\mathbf{w}_{i+1} = \\mathbf{w}_i - \\alpha_i \\sum_j (\\mathbf{w}_i^\\top\\mathbf{x}_j  - y_j) \\mathbf{x}_j \\,.\\\\]\n",
    "where \\\\( \\scriptsize i \\\\) is the iteration number of the gradient descent algorithm, and \\\\( \\scriptsize j \\\\) identifies the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_summand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        weights (DenseVector): An array of model weights (betas).\n",
    "        lp (LabeledPoint): The `LabeledPoint` for a single observation.\n",
    "\n",
    "    Returns:\n",
    "        DenseVector: An array of values the same length as `weights`.  The gradient summand.\n",
    "    \"\"\"\n",
    "    gradient_summand = (((weights.transpose()).dot(lp.features)) - lp.label) * (lp.features)\n",
    "    return gradient_summand\n",
    "\n",
    "example_w = DenseVector([1, 1, 1])\n",
    "example_lp = LabeledPoint(2.0, [3, 1, 4])\n",
    "# gradient_summand = (dot([1 1 1], [3 1 4]) - 2) * [3 1 4] = (8 - 2) * [3 1 4] = [18 6 24]\n",
    "summand_one = gradient_summand(example_w, example_lp)\n",
    "print summand_one\n",
    "\n",
    "example_w = DenseVector([.24, 1.2, -1.4])\n",
    "example_lp = LabeledPoint(3.0, [-1.4, 4.2, 2.1])\n",
    "summand_two = gradient_summand(example_w, example_lp)\n",
    "print summand_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use weights to make predictions.\n",
    "def get_labeled_prediction(weights, observation):\n",
    "    \"\"\"Calculates predictions and returns a (prediction, label) tuple.\n",
    "\n",
    "    Note:\n",
    "        The labels should remain unchanged as we'll use this information to calculate prediction\n",
    "        error later.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): An array with one weight for each features in `trainData`.\n",
    "        observation (LabeledPoint): A `LabeledPoint` that contain the correct label and the\n",
    "            features for the data point.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A (prediction, label) tuple. Convert the return type of the label and prediction to a float.\n",
    "    \"\"\"\n",
    "    predictions = weights.dot(observation.features)\n",
    "    labels = observation.label\n",
    "    return(float(predictions), float(labels))\n",
    "\n",
    "weights = np.array([1.0, 1.5])\n",
    "prediction_example = sc.parallelize([LabeledPoint(2, np.array([1.0, .5])),\n",
    "                                     LabeledPoint(1.5, np.array([.5, .5]))])\n",
    "preds_and_labels_example = prediction_example.map(lambda lp: get_labeled_prediction(weights, lp))\n",
    "print preds_and_labels_example.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement a gradient descent function for linear regression.\n",
    "def linreg_gradient_descent(train_data, num_iters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        train_data (RDD of LabeledPoint): The labeled data for use in training the model.\n",
    "        num_iters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The length of the training data\n",
    "    n = train_data.count()\n",
    "    # The number of features in the training data\n",
    "    d = len(train_data.first().features)\n",
    "    w = np.zeros(d)\n",
    "    alpha = 1.0\n",
    "    # We will compute and store the training error after each iteration\n",
    "    error_train = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        # Use get_labeled_prediction from (3b) with trainData to obtain an RDD of (label, prediction)\n",
    "        # tuples.  Note that the weights all equal 0 for the first iteration, so the predictions will\n",
    "        # have large errors to start.\n",
    "        preds_and_labels_train = train_data.map(lambda lp: get_labeled_prediction(w, lp))\n",
    "        preds_and_labels_train_df = sqlContext.createDataFrame(preds_and_labels_train, [\"prediction\", \"label\"])\n",
    "        error_train[i] = calc_RMSE(preds_and_labels_train_df)\n",
    "\n",
    "        # Calculate the `gradient`.  Make use of the `gradient_summand` function you wrote in (3a).\n",
    "        # Note that `gradient` should be a `DenseVector` of length `d`.\n",
    "        gradient = train_data.map(lambda lp: gradient_summand(w, lp)).sum()\n",
    "\n",
    "        # Update the weights\n",
    "        alpha_i = alpha / (n * np.sqrt(i+1))\n",
    "        w -= alpha_i*gradient\n",
    "    return w, error_train\n",
    "\n",
    "# create a toy dataset with n = 10, d = 3, and then run 5 iterations of gradient descent\n",
    "# note: the resulting model will not be useful; the goal here is to verify that\n",
    "# linreg_gradient_descent is working properly\n",
    "example_n = 10\n",
    "example_d = 3\n",
    "example_data = (sc\n",
    "                 .parallelize(parsed_train_data_df.take(example_n))\n",
    "                 .map(lambda lp: LabeledPoint(lp.label, lp.features[0:example_d])))\n",
    "print example_data.take(2)\n",
    "example_num_iters = 5\n",
    "example_weights, example_error_train = linreg_gradient_descent(example_data, example_num_iters)\n",
    "print example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's train a linear regression model on all of our training data and evaluate its accuracy on the validation set\n",
    "num_iters = 50\n",
    "weights_LR0, error_train_LR0 = linreg_gradient_descent(parsed_train_data_df, num_iters)\n",
    "\n",
    "preds_and_labels = (parsed_val_data_df\n",
    "                      .map(lambda lp: get_labeled_prediction(weights_LR0, lp)))\n",
    "preds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\n",
    "rmse_val_LR0 = calc_RMSE(preds_and_labels_df)\n",
    "\n",
    "print 'Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(rmse_val_base,\n",
    "                                                                       rmse_val_LR0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Values to use when training the linear regression model\n",
    "\n",
    "num_iters = 500  # iterations\n",
    "reg = 1e-1  # regParam\n",
    "alpha = .2  # elasticNetParam\n",
    "use_intercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\n",
    "first_model = lin_reg.fit(parsed_train_data_df)\n",
    "\n",
    "# coeffsLR1 stores the model coefficients; interceptLR1 stores the model intercept\n",
    "coeffs_LR1 = first_model.coefficients\n",
    "intercept_LR1 = first_model.intercept\n",
    "print coeffs_LR1, intercept_LR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "sample_prediction = first_model.transform(parsed_train_data_df)\n",
    "display(sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate RMSE\n",
    "val_pred_df = first_model.transform(parsed_val_data_df)\n",
    "rmse_val_LR1 = calc_RMSE(val_pred_df)\n",
    "\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}' +\n",
    "       '\\n\\tLR1 = {2:.3f}').format(rmse_val_base, rmse_val_LR0, rmse_val_LR1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform grid search to find a good regularization parameter\n",
    "best_RMSE = rmse_val_LR1\n",
    "best_reg_param = reg\n",
    "best_model = first_model\n",
    "\n",
    "num_iters = 500  # iterations\n",
    "alpha = .2  # elasticNetParam\n",
    "use_intercept = True  # intercept\n",
    "\n",
    "for reg in [1e-10, 1e-5, 1.0]:\n",
    "    lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\n",
    "    model = lin_reg.fit(parsed_train_data_df)\n",
    "    val_pred_df = model.transform(parsed_val_data_df)\n",
    "\n",
    "    rmse_val_grid = calc_RMSE(val_pred_df)\n",
    "    print rmse_val_grid\n",
    "\n",
    "    if rmse_val_grid < best_RMSE:\n",
    "        best_RMSE = rmse_val_grid\n",
    "        best_reg_param = reg\n",
    "        best_model = model\n",
    "\n",
    "rmse_val_LR_grid = best_RMSE\n",
    "\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n' +\n",
    "       '\\tLRGrid = {3:.3f}').format(rmse_val_base, rmse_val_LR0, rmse_val_LR1, rmse_val_LR_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will add features that capture the two-way interactions between our existing features\n",
    "import itertools\n",
    "\n",
    "def two_way_interactions(lp):\n",
    "    \"\"\"Creates a new `LabeledPoint` that includes two-way interactions.\n",
    "\n",
    "    Note:\n",
    "        For features [x, y] the two-way interactions would be [x^2, x*y, y*x, y^2] and these\n",
    "        would be appended to the original [x, y] feature list.\n",
    "\n",
    "    Args:\n",
    "        lp (LabeledPoint): The label and features for this observation.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The new `LabeledPoint` should have the same label as `lp`.  Its features\n",
    "            should include the features from `lp` followed by the two-way interaction features.\n",
    "    \"\"\"\n",
    "    combinations = itertools.product(lp.features, repeat = 2)\n",
    "    list_combs = []\n",
    "    final_list = [lp.label]\n",
    "    for y in lp.features:\n",
    "      list_combs.append(y)\n",
    "    for x in combinations:\n",
    "      prods = np.prod(np.array(x))\n",
    "      list_combs.append(prods)\n",
    "    final_list.append(list_combs)\n",
    "    lbp = LabeledPoint(label=final_list[0], features=final_list[1])\n",
    "    return lbp\n",
    "\n",
    "print two_way_interactions(LabeledPoint(0.0, [2, 3]))\n",
    "\n",
    "# Transform the existing train, validation, and test sets to include two-way interactions.\n",
    "# Remember to convert them back to DataFrames at the end.\n",
    "train_data_interact_df = parsed_train_data_df.map(two_way_interactions).toDF()\n",
    "val_data_interact_df = parsed_val_data_df.map(two_way_interactions).toDF()\n",
    "test_data_interact_df = parsed_test_data_df.map(two_way_interactions).toDF()\n",
    "a=parsed_val_data_df.rdd.first()\n",
    "print a.features\n",
    "f=two_way_interactions(LabeledPoint(a.label, a.features)).features\n",
    "print sum(f)\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets build the new model.\n",
    "num_iters = 500\n",
    "reg = 1e-10\n",
    "alpha = .2\n",
    "use_intercept = True\n",
    "\n",
    "lin_reg = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha, fitIntercept=use_intercept)\n",
    "model_interact = lin_reg.fit(train_data_interact_df)\n",
    "preds_and_labels_interact_df = model_interact.transform(val_data_interact_df)\n",
    "rmse_val_interact = calc_RMSE(preds_and_labels_interact_df)\n",
    "\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n\\tLRGrid = ' +\n",
    "       '{3:.3f}\\n\\tLRInteract = {4:.3f}').format(rmse_val_base, rmse_val_LR0, rmse_val_LR1,\n",
    "                                                 rmse_val_LR_grid, rmse_val_interact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate interaction model on test data.\n",
    "preds_and_labels_test_df = model_interact.transform(test_data_interact_df)\n",
    "rmse_test_interact = calc_RMSE(preds_and_labels_test_df)\n",
    "\n",
    "print ('Test RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLRInteract = {1:.3f}'\n",
    "       .format(rmse_test_base, rmse_test_interact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a pipeline to create the interaction model.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "num_iters = 500\n",
    "reg = 1e-10\n",
    "alpha = .2\n",
    "use_intercept = True\n",
    "\n",
    "polynomial_expansion = PolynomialExpansion(degree=2, inputCol='features', outputCol='polyFeatures')\n",
    "linear_regression = LinearRegression(maxIter=num_iters, regParam=reg, elasticNetParam=alpha,\n",
    "                                     fitIntercept=use_intercept, featuresCol='polyFeatures')\n",
    "\n",
    "pipeline = Pipeline(stages=[polynomial_expansion, linear_regression])\n",
    "pipeline_model = pipeline.fit(parsed_train_data_df)\n",
    "\n",
    "predictions_df = pipeline_model.transform(parsed_test_data_df)\n",
    "\n",
    "evaluator = RegressionEvaluator()\n",
    "rmse_test_pipeline = evaluator.evaluate(predictions_df, {evaluator.metricName: \"rmse\"})\n",
    "print('RMSE for test data set using pipelines: {0:.3f}'.format(rmse_test_pipeline))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "cs120_lab2_linear_regression_df",
  "notebookId": 2614515414930341
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
